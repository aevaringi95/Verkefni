---
title: "10.9"
author: "Ævar Ingi Jóhannesson"
date: "3/2/2018"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gridExtra)
library(class)
library(data.table)
library(ggpubr)
library(prettydoc)
library(knitr)
library(lattice)
library(ggplot2)
library(evd)
library(MASS)
library(LaplacesDemon)
library(grid)
library(gridExtra)
library(cowplot)
library(ISLR)
library(dplyr)
library(leaps)
library(glmnet)
library(pls)
library(stats)
library(boot)
library(splines)
library(gam)
library(tree)
library(randomForest)
library(gbm)
library(e1071)

```

## Problem.
Here we will perform the unsupervised learning technice called hierarchical clustering on the states of the USArrest data set. The data set contains 50 observations(states) and 4 variables.


### a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.


```{r echo=TRUE}
set.seed(5)
dataUS <- USArrests
hcComp <- hclust(dist(dataUS),method="complete")
```

Here we fit a hierarchical clustering on the USArrest data set with Euclidean distance and complete linkage.
Complete linkage is the "worst case scenario" it means that it's the farthest distance from a data point in a group A to a data point in B.

### b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r,echo=TRUE}
plot(hcComp,main="Complete Linkage", xlab="", sub="",
cex =.9)

hcCompCut<- cutree(hcComp, 3)
name <- names(sort(hcCompCut))
numbers <- unname(sort(hcCompCut))
tabledf <- cbind(name,numbers)
kable(tabledf,col.names = c('State:','Group:'))

```

Here we can see the states in each cluster and the dendogram. From the dendogram we can see that it's a good idea to cut the tree such that we get three clusters.

### c) Doing the same but scaling the data set first.


```{r echo=TRUE}
set.seed(5)
scaleUS <- as.data.frame(scale(USArrests))
sd(scaleUS$Assault)
mean(scaleUS$Assault)
hcCompScale <- hclust(dist(scaleUS),method="complete")
```

We can see that the standard deviation is 1 and the mean is almost 0 for the Assault column.

```{r,echo=TRUE}
plot(hcCompScale,main="Complete Linkage", xlab="", sub="",
cex =.9)
```

We can see that it effects the data. There seems to be that we should rather split the data into 4 groups then three.

```{r,echo=TRUE}
hcCompCutScale<- cutree(hcCompScale, 3)
name2 <- names(sort(hcCompCutScale))
numbers2 <- unname(sort(hcCompCutScale))
tabledf2 <- cbind(name2,numbers2)
kable(tabledf2,col.names = c('State:','Group:'))

```

Here we can see in the table above that it is not putting the same states in the same clusters as it did before scaling.


### d) What effect does scaling have on the hierarchical clustering.

We should scale the variables if we are using Euclidean distance and the measure of units in a certain column is very different from the others. For example in this data set we can see that the units are different. Assault has much higher number then the others. If we don't scale we can put weight on a certain unit so it will have more effect on the clustering then the other variables.

But ofcourse if you want one predictior to have more effect on the others you don't have to scale the variables. It often just depends on the data. When we are performing clustering it's important to look at the data and understand what it is measuring and what you want out of the clustering.


#### Closer look at the Assault column

```{r,echo=TRUE}
ind <- sort.int(dataUS[,2],decreasing=T,index.return = TRUE)
dataUS[ind$ix[1:10],]

ind2 <- sort.int(dataUS[,2],decreasing=F,index.return = TRUE)
dataUS[ind2$ix[1:10],]

```

Here we can see that the highest 10 states of assault corresponds to group number 1 in the clustering without scaling and the lowest 10 states of assault correspond to group number 3. So that's an indication that Assault has much effect on the first splits in the clustering without scale.

If we look at the second clustering we can see that Assault is not grouping the data the same way as before. That means that the other variables of the data are having more effect
