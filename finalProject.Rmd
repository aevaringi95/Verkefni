---
title: "Final Project"
author: "Rakel María Brynjólfsdóttir og Ævar Ingi Jóhannesson"
date: "March 23, 2018"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

```{r, warning = F, message = F}
library(plyr); library(dplyr); library(lubridate)
library(ggplot2)
library(grid)
library(gridExtra)
library(glmnet)
```

```{r}
dataO = read.csv("gagnasafn_ibudarhusnaedi.csv",
                header  = T,
                sep = ";",
                dec = ",")
data = dataO #svo ég þurfi ekki að keyra allt aftur, hendum þessu út seinna
```

###Data preprocessing

We start by removing all the data which is not in the capital region. 

```{r}
capitalArea = c(0000,1000,1100,1300,1400,1604,1606)
data = data[data$svfn %in% capitalArea,]
```

Now we are going to remove the variables that we won't be using.

```{r}
delete = names(data)[c(1,2,5,6,7,10,11,12,13,14,15,17,18,19,20,21,
                       25,26,30,37,38,40,41,42,44,47,50,52,53)]
data = data[,-which(names(data) %in% delete)]
```

Some variable have many levels and few observations on some levels so we are going to remove those observations. 

```{r}
#Einbylishus, Fjolbylishus, Ibudareign, Ibudarhus, Parhus, Radhus. Getum fækkað meira 
lev = levels(data$teg_eign)[c(1,2,7,8,11)] 
data = data[which(data$teg_eign %in% lev),]
data$teg_eign = droplevels(data$teg_eign)

data$matssvaedi = as.factor(data$matssvaedi)
lev = levels(data$matssvaedi)[c(61,62)]
data = data[-which(data$matssvaedi %in% lev),]
data$matssvaedi = droplevels(data$matssvaedi)

data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
lev = levels(data$undirmatssvaedi)[c(10,36,38,39,50,55)]
data = data[-which(data$undirmatssvaedi %in% lev),]
data$undirmatssvaedi = droplevels(data$undirmatssvaedi)
```

Many of the numerical variables should be factors, and some factors should be numerical. We are also going to combine some of the levels for certain factors since the higher levels have very few observations in many cases.

```{r}
data$svfn = as.factor(data$svfn)
data$lyfta[which(data$lyfta != 0)] = 1
data$lyfta = as.factor(data$lyfta)
data$fjsturt[which(!(data$fjsturt %in% c(0,1)))] = 2
data$fjsturt = as.factor(data$fjsturt)
data$fjklos[which(!(data$fjklos %in% c(0,1)))] = 2
data$fjklos = as.factor(data$fjklos)
data$fjbkar[which(!(data$fjbkar %in% c(0,1)))] = 2
data$fjbkar = as.factor(data$fjbkar)
data$fjeld[which(!(data$fjeld %in% c(0,1)))] = 2
data$fjeld = as.factor(data$fjeld)
data$fjstof[which(!(data$fjstof %in% c(0,1)))] = 2
data$fjstof = as.factor(data$fjstof)
data$fjhaed = as.factor(data$fjhaed)
data$fjherb[which(!(data$fjherb %in% c(0,1,2,3,4,5)))] = 6
data$fjherb = as.factor(data$fjherb)
data$svalm2 = as.numeric(data$svalm2)
data$fjmib = as.factor(data$fjmib)
data$ibteg = as.factor(data$ibteg)
data$svfn = revalue(data$svfn, c("0"="Rvk","1000"="Kop",
                                 "1100"="Sel","1300"="Grb",
                                 "1400"="Hfj","1604"="Mos"))
data$ibteg = revalue(data$ibteg, c("11"="1","12"="2"))
data$byggar = droplevels(data$byggar)
```

We are going to change the kdagur variable so that we have the days since the property was bought. 

```{r}
data$kdagur <- strptime(x = as.character(data$kdagur),
                                format = "%d.%m.%Y")
mydate <- as.Date("2018-04-01")
data$kdagur <- as.Date(data$kdagur)
data$kdagur <- as.numeric(mydate-data$kdagur)
```

And we're going to change the byggar variable into the age of the house in years. 

```{r}
data$byggar <- 2018 - as.numeric(as.character(data$byggar))
colnames(data)[5] = "aldur"
```

Now we remove 0 values for nuvirdi and check for NA values. 
```{r}
noNuvirdi <- which(data$nuvirdi==0)
data = data[-noNuvirdi,]
dim(data) == dim(na.omit(data))
```

Now we'll check whether any of the numerical variables are highly correlated. 

```{r}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,ntm2,svalm2,geymm2))
cor(X)
```

The ibm2 and ntm2 have a very high correlation so I'm removing one of them. 

```{r}
data$ntm2 = NULL 
```

### Descriptive plots of the data


Here we look at the density of the Nuvirdi function. When we log scale it it looks a lot more like normal density.
```{r}
density1 <- ggplot(data,aes(nuvirdi))+
  geom_density()

density2 <- ggplot(data,aes(log(nuvirdi)))+
  geom_density()

grid.arrange(density1,density2,nrow=2)
data$nuvirdi = log(data$nuvirdi)
```

Here we set nuvirdi log transformes. Let's plot nuvirdi against some variables.

```{r}
p1 <- ggplot(data, aes(x=kdagur,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey()

p2 <- ggplot(data, aes(x=stig10,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p3 <- ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p4 <- ggplot(data, aes(x=ummal,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p5 <- ggplot(data, aes(x=aldur,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p6 <- ggplot(data, aes(x=bilskurm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p7 <- ggplot(data, aes(x=svalm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p8 <- ggplot(data, aes(x=ibm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

grid.arrange(p1,p2,p3,p4,nrow = 2)
grid.arrange(p5,p6,p7,p8,nrow = 2)
```

Let's plot geymm2, bilskurm2 and ibm2 with log transformations. 

```{r}
B = data$bilskurm2
ind1 = which(B == 0)
B[-ind1] = log(B[-ind1])
I = log(data$ibm2)
G = data$geymm2
ind3 = which(G == 0)
G[-ind3] = log(G[-ind3])
```

```{r}
p1 <- ggplot(data, aes(x=B,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey() + labs(x="bilskurm2")

p2 <- ggplot(data, aes(x=I,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="ibm2")

p3 <- ggplot(data, aes(x=G,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="geymm2")

grid.arrange(p1,p2,p3,nrow = 2)
```

A log transformation is not suitable for the bilkskurm2 variable, but it definitely is for ibm2 and geymm2, although we will have to fix the values that are at zero. 

```{r}
data$ibm2 = log(data$ibm)

G[ind3] = G[ind3] - 1.8 #veit ekki hvort þetta sé góð lausn en gerði svipað í bayes
data$geymm2 = G
ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() 
```


Now we'll make some box plots of the factor variables. 

```{r}
f1 <- ggplot(data,aes(x=teg_eign,y=nuvirdi))+
          geom_boxplot()

f2 <- ggplot(data,aes(x=svfn,y=nuvirdi))+
          geom_boxplot()

f3 <- ggplot(data,aes(x=fjmib,y=nuvirdi))+
          geom_boxplot()

f4 <- ggplot(data,aes(x=lyfta,y=nuvirdi))+
          geom_boxplot()

f5 <- ggplot(data, aes(x=fjhaed,y=nuvirdi)) +
          geom_boxplot()

f6 <- ggplot(data,aes(x=fjbkar,y=nuvirdi))+
          geom_boxplot()

f7 <- ggplot(data,aes(x=fjsturt,y=nuvirdi))+
          geom_boxplot()

f8 <- ggplot(data,aes(x=fjklos,y=nuvirdi))+
          geom_boxplot()

f9 <- ggplot(data,aes(x=fjeld,y=nuvirdi))+
          geom_boxplot()

f10 <- ggplot(data,aes(x=fjherb,y=nuvirdi))+
          geom_boxplot()

f11 <- ggplot(data, aes(x=fjstof,y=nuvirdi)) +
          geom_boxplot()

f12 <- ggplot(data, aes(x=ibteg,y=nuvirdi)) +
          geom_boxplot()

grid.arrange(f1,f2,f3,
             f4,f5,
             f6,f7,
             f8,f9,f10,f11,f12,ncol=3)
```

Now that we are happy with the data we'll check the correlation again. 

```{r}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,svalm2,geymm2))
cor(X)
```

Now the data is as follows: 

  Variable      | Description
--------------- | -----------
kdagur          | days since property was bought
nuvirdi         | log of current price
teg_eign        | type of property (6 levels)
svfn            | town id (6 levels)
aldur           | age of house
fjmib           | number of apartments in building (4 levels)
lyfta           | whether house has elevator or not (2 levels)
ummal           | circumference of property 
ibm2            | log of area of property
ntm2            | netto area of property
fjhaed          | number of floors in building (4 levels)
fjbkar          | number of bathtubs (3 levels)
fjsturt         | number of showers (3 levels)
fjklos          | number of toilets (3 levels)
fjeld           | number of kitchens (3 levels)
fjherb          | number of rooms (7 levels)
fjstof          | number of livingrooms (3 levels)
stig10          | construction stage 
bilskurm2       | area of garage
svalm2          | area of balcony
geymm2          | log of area of storage space 
matssvaedi      | location (60 levels)
undirmatssvaedi | sub-location (63 levels)
ibteg           | type of property (2 levels)

```{r}
#write.csv(data, file = "MyData.csv",row.names = F)
```

Jesús, write.csv viðheldur ekki hvaða breytur eru factor og hverjar eru numerical. Þurfum að finna útúr þessu. 

```{r}
#rm(list = ls())
#dat2 = read.csv("MyData.csv",as.is = T)
set.seed(8756)
dat = data
svfn = dat$svfn
dat$svfn = NULL
n = dim(dat)[1]
train = sample(n,floor(n*0.8)) #getum haft eitthvað annað split
dat.train = dat[train,]
dat.test = dat[-train,]
```


## Linear Regression

```{r}
fit.lm = lm(nuvirdi~.,dat.train)
summary(fit.lm)
pred.lm = predict(fit.lm, dat.test)
RSS1 = mean((dat.test$nuvirdi - pred.lm)^2)
Rss = mean((exp(dat.test$nuvirdi)-exp(pred.lm))^2)
Rss
RSS1

```

####Diagnostics of linear regression
Here below we can see some plots for the linear regression model.

```{r}
diagnost = fortify(fit.lm)
diagnost$.index = 1:(dim(diagnost)[1])
res_sd = sd(diagnost$.resid)
resPlot = ggplot(diagnost, aes(x=seq(1:length(.resid)),y=.resid)) +
                geom_point()+
                geom_hline(yintercept=0, col="red", linetype="dashed")+
                geom_hline(yintercept=2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=-2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=3*res_sd, col="green", linetype="dashed")+
                geom_hline(yintercept=-3*res_sd, col="green", linetype="dashed")+
                xlab("Index")+ylab("Residuals")+labs(title = "Index plot of residuals")+
                geom_text(aes(label=ifelse(abs(.resid)>3*res_sd,.index,"")),hjust=-.5, vjust=0.4)+
                theme_grey()
resPlot
```

Here we can see the residuals plotted against their index. The blue line is 2 times the standard deviation of the residuals and the green line is 3 times the standard deviation. We can see that there are few points that exceed the green line and they could be looked at more closely. Also we can see that there seems to be a constant variance.

```{r}
QQPlot<-ggplot(diagnost, aes(sample = .resid))+
        stat_qq()+
        geom_abline(slope=res_sd)+
        ylab("Residuals")+
        theme_grey()
QQPlot
```

Here we can see the normal Q-Q plot for the model. It looks good and they seem to follow a normal distribution but the ends are more spread out. They are heavi-tailed.

```{r}
LevPlot <- ggplot(diagnost, aes(x=seq(1:length(.hat)),y=.hat))+
                  geom_point()+
                  geom_hline(yintercept=4*mean(diagnost$.hat), col="red", linetype="dashed")+
                  xlab("Index")+
                  ylab("Leverages")+
                  geom_text(aes(label=ifelse(.hat > 4*mean(diagnost$.hat),.index,"")),hjust=0, vjust=0)+
                  theme_grey()
LevPlot
```

Here we can see the Leverage plot. There seems to be one point that has very high leverage. This point is in undirmatssvæði 101 which has only 2 observations and there for it has this high leverage. So it doesn't mean that it's an outlier.



##Best subset selection.

Here we can see that R adjusted.
```{r}
library(leaps)
regfit.fwd <- regsubsets(nuvirdi ~ ., data = dat.train, 
    nvmax = 150,method="forward")
for.summary <- summary(regfit.fwd)
which.max(for.summary$adjr2)

ggplot(data.frame(y=for.summary$adjr2, x=1:150), aes(x,y))+geom_point() + ylab("AdjR2") + xlab("Subset size")
```


## Lasso Regression

Mögulega þarf að scale-a breytti ekki miklu samt. Lasso er samt frekar lelegt miðað við linear.
```{r}
ytrain = data.matrix(dat.train[,"nuvirdi"])
Xtrain = data.matrix(dat.train[,!(colnames(dat.train) %in% c("nuvirdi"))])
Xtest = data.matrix(dat.test[,!(colnames(dat.train) %in% c("nuvirdi"))])
grid <- 10^seq(1, -20, length=1000)
fit.lasso = cv.glmnet(Xtrain,ytrain,alpha=1, lambda=grid, thresh=1e-12)
plot(fit.lasso)
best.lambda = fit.lasso$lambda.min
pred.lasso = predict(fit.lasso,Xtest,s=best.lambda)
RSS2 <- mean((dat.test$nuvirdi - pred.lasso)^2)
RssNormal2 <- mean((exp(dat.test$nuvirdi)-exp(pred.lasso))^2)
RSS2
RssNormal2

```

Here we can see that when we do Lasso regression we can see that the MSE is much higher then for a normal linear regression.


```{r}
best.model = glmnet(Xtrain, ytrain, alpha = 1)
predict(best.model, s = fit.lasso$lambda.1se, type = "coefficients")
```

The first two predictors to tend to zero are fjeld and svalm2.


##Boosting

Here we perform Boosting.

```{r}
library(gbm)
set.seed(50550)
lambd <-  seq(0.001, 0.02, by=0.001)
lambd <- c(lambd, seq(0.02, 0.5, by=0.01))
m <- length(lambd)
testErr <- rep(NA, m)
for (i in 1:m) {
    boostCol <- gbm(nuvirdi ~ ., data = dat.train,
                    distribution = "gaussian",
                    n.trees = 200, 
                    shrinkage = lambd[i])
    testPred <- predict(boostCol, dat.test, n.trees = 200)
    testErr[i] = mean((dat.test$nuvirdi - testPred)^2)
    print(i)
}

bestLam = lambd[which.min(testErr)]

ggplot(data.frame(x=lambd, y=testErr), aes(x=x, y=y)) +
      xlab("Shrinkage") + 
      ylab("Test MSE") + 
      geom_point()


boost.fit <- gbm(nuvirdi ~ ., data = dat.train, distribution = "gaussian", n.trees = 1000, shrinkage =bestLam )
boost.pred <- predict(boost.fit, dat.test, n.trees = 1000)
RssBoos = mean((dat.test$nuvirdi - boost.pred)^2)
Rss2Boost = mean((exp(dat.test$nuvirdi) - exp(boost.pred))^2)

Rss2Boost
RssBoos

```




### Classification 

Here we are going to predict in which town these properties belong to.

The classes are 
Rvk
Kop
Hfj

Mögulega ættum við að hafa bara 3. Þau tala um það i verkefnalysingunni.

```{r}
str(dat)
dat = data.frame(dat,svfn)
mats = dat$matssvaedi
undir = dat$undirmatssvaedi
dat$matssvaedi = NULL
dat$undirmatssvaedi = NULL

#Höfum bara Rvk, Kop og Hfj. 
area = levels(data$svfn)[c(1,2,5)]
dat = dat[which(data$svfn %in% area),]
dat$svfn = droplevels(dat$svfn)

n = dim(dat)[1]
p = dim(dat)[2]
train = sample(n,floor(n*0.8)) #getum haft eitthvað annað split
dat.train = dat[train,]
dat.test = dat[-train,]
```

## Random Forest

```{r}
library(randomForest)
set.seed(4949)
err = integer()
mtrys = seq(from = 2, to = 20, by = 1)
for(i in mtrys){
  fit.rf = randomForest(svfn~., dat.train, mtry = i, ntree = 200) 
  pred.rf = predict(fit.rf, dat.test)
  err[i-1] = mean(dat.test$svfn != pred.rf)
  print(i)
}

ggplot(data=data.frame(x=mtrys,y=err),aes(x=x,y=y))+
  xlab("nr of variables considered at each split")+
  ylab("MSE")+
  geom_point()

min(err)
minErr = which.min(err)
fit.rf = randomForest(svfn~., dat.train, mtry = minErr, ntree = 200)
pred.rf = predict(fit.rf, dat.test)
err = mean(dat.test$svfn != pred.rf)
err
fit.rf
importance(fit.rf) #HERE we can see the most important variables
```

## Support Vector Machine
 
```{r}
library(e1071)
fit.svm = svm(svfn~.,data = dat.train,scale = T, kernel = "radial", cost = 200,gamma=0.1) #gamma
pred.svm = predict(fit.svm,dat.test)
err.svm = mean(pred.svm!=dat.test$svfn)
err.svm

tune.svm = tune(svm, svfn~.,data = dat.train, scale=T, kernel = "radial", ranges = list(cost = c(50,100,200), gamma = 0.1))#Cost 5, Gamma = 0.5
summary(tune.svm)

fit.svm2 = svm(svfn~.,data = dat.train,scale = T, kernel = "radial", cost = 5,gamma=0.5)
pred.svm2 = predict(fit.svm2,dat.test)
err.svm2 = mean(pred.svm2 != dat.test$svfn)
err.svm2
```
