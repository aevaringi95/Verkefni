---
title: "Final Project"
author: "Rakel María Brynjólfsdóttir og Ævar Ingi Jóhannesson"
date: "March 23, 2018"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

```{r, warning = F, message = F}
library(plyr); library(dplyr); library(lubridate)
library(ggplot2)
library(grid)
library(gridExtra)
library(glmnet)
library(e1071)
library(randomForest)
library(knitr)
library(gbm)
```

We start by loading the data which has already been cleaned. The particulars of the data preproccesing can be seen in the appendix. 

```{r}
data = read.csv("MyData.csv")
dataOriginal = data
```

```{r, echo = F}
data$fjmib = as.factor(data$fjmib)
data$lyfta = as.factor(data$lyfta)
data$fjhaed = as.factor(data$fjhaed)
data$fjbkar = as.factor(data$fjbkar)
data$fjsturt = as.factor(data$fjsturt)
data$fjklos = as.factor(data$fjklos)
data$fjeld = as.factor(data$fjeld)
data$fjherb = as.factor(data$fjherb)
data$fjstof = as.factor(data$fjstof)
data$matssvaedi = as.factor(data$matssvaedi)
data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
data$ibteg = as.factor(data$ibteg)
```

The data is now as follows: 

  Variable     |  type     | Description               |  levels
-------------- | --------- | ------------------------- | ----------------------- 
kdagur         | numerical | days since last bought    | 
nuvirdi        | numerical | log of current price      | 
teg_eign       | factor    | type of property          | Einbýlishús, Fjölbýlishús, Íbúðareign, Íbúðarhús, Raðhús
svfn           | factor    | town id                   | Grb, Hfj, Kop, Mos, Rvk, Sel
aldur          | numerical | age of house              |   
fjmib          | factor    | nr of apartments          | 1,2,3,4
lyfta          | factor    | elevator or not           | 0,1
ummal          | numerical | circumference of property | 
ibm2           | numerical | area of property          |
ntm2           | numerical | netto area of property    |
fjhaed         | factor    | number of floors          | 1,2,3,4
fjbkar         | factor    | number of bathtubs        | 0,1,2 (2 or more)
fjsturt        | factor    | number of showers         | 0,1,2 (2 or more)
fjklos         | factor    | number of toilets         | 0,1,2 (2 or more)
fjeld          | factor    | number of kitchens        | 0,1,2 (2 or more)
fjherb         | factor    | number of rooms           | 0,1,2,3,4,5,6 (6 or more)
fjstof         | factor    | number of livingrooms     | 0,1,2 (2 or more)
stig10         | numerical | construction stage        |
bilskurm2      | numerical | area of garage            |
svalm2         | numerical | area of balcony           |
geymm2         | numerical | area of storage space     |
matssvaedi     | factor    | location                  | 60 levels
undirmatssvaedi| factor    | sub-location              | 62 levels
ibteg          | factor    | type of property          | 1(single-family), 2(aparment building)

### Descriptive plots of the data

We start by looking at the density of the $\texttt{nuvirdi}$ variable.  

```{r}
density1 <- ggplot(data,aes(nuvirdi))+
  geom_density()

density2 <- ggplot(data,aes(log(nuvirdi)))+
  geom_density()

grid.arrange(density1,density2,nrow=2)
```

It's evident that when we plot $\texttt{nuvirdi}$ on a log scale it looks a lot more like the normal distribution, so we're going to log transform the response. 

```{r}
data$nuvirdi = log(data$nuvirdi)
```

Let's plot $\texttt{nuvirdi}$ against some variables to see if there are any relationships. 

```{r}
p1 <- ggplot(data, aes(x=kdagur,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey()

p2 <- ggplot(data, aes(x=stig10,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p3 <- ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p4 <- ggplot(data, aes(x=ummal,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p5 <- ggplot(data, aes(x=aldur,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p6 <- ggplot(data, aes(x=bilskurm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p7 <- ggplot(data, aes(x=svalm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p8 <- ggplot(data, aes(x=ibm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

grid.arrange(p1,p2,p3,p4,nrow = 2)
grid.arrange(p5,p6,p7,p8,nrow = 2)
```

It looks like $\texttt{geymm2}$, $\texttt{bilskurm2}$ and $\texttt{ibm2}$ might have a more linear relatioship with nuvirdi on a log scale. 

```{r}
B = data$bilskurm2
ind1 = which(B == 0)
B[-ind1] = log(B[-ind1])
I = log(data$ibm2)
G = data$geymm2
ind3 = which(G == 0)
G[-ind3] = log(G[-ind3])
```

```{r}
p1 <- ggplot(data, aes(x=B,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey() + labs(x="bilskurm2")

p2 <- ggplot(data, aes(x=I,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="ibm2")

p3 <- ggplot(data, aes(x=G,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="geymm2")

grid.arrange(p1,p2,p3,nrow = 2)
```

A log transformation is not suitable for the $\texttt{bilskurm2}$ variable, but it definitely is for $\texttt{ibm2}$ and $\texttt{geymm2}$, although we will have to shift the values so that they are positive. 

```{r}
data$ibm2 = log(data$ibm)
#breytti þessu þannig að allt er jákvætt. 
#G[ind3] = G[ind3] - 1.8 
G[-ind3] = G[-ind3] + 1.7
data$geymm2 = G
ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() 
```


Now we'll make some box plots of the factor variables. 

```{r}
f1 <- ggplot(data,aes(x=teg_eign,y=nuvirdi))+
          geom_boxplot()

f2 <- ggplot(data,aes(x=svfn,y=nuvirdi))+
          geom_boxplot()

f3 <- ggplot(data,aes(x=fjmib,y=nuvirdi))+
          geom_boxplot()

f4 <- ggplot(data,aes(x=lyfta,y=nuvirdi))+
          geom_boxplot()

f5 <- ggplot(data, aes(x=fjhaed,y=nuvirdi)) +
          geom_boxplot()

f6 <- ggplot(data,aes(x=fjbkar,y=nuvirdi))+
          geom_boxplot()

f7 <- ggplot(data,aes(x=fjsturt,y=nuvirdi))+
          geom_boxplot()

f8 <- ggplot(data,aes(x=fjklos,y=nuvirdi))+
          geom_boxplot()

f9 <- ggplot(data,aes(x=fjeld,y=nuvirdi))+
          geom_boxplot()

f10 <- ggplot(data,aes(x=fjherb,y=nuvirdi))+
          geom_boxplot()

f11 <- ggplot(data, aes(x=fjstof,y=nuvirdi)) +
          geom_boxplot()

f12 <- ggplot(data, aes(x=ibteg,y=nuvirdi)) +
          geom_boxplot()

grid.arrange(f1,f2,f3,
             f4,f5,
             f6,f7,
             f8,f9,f10,f11,f12,ncol=3)
```

Most of the factor variables seem to have a relationship with $\texttt{nuvirdi}$, especially $\texttt{fjmib}$, $\texttt{fjhaed}$ and $\texttt{fjherb}$. 

Finally, we'll check the correlation between the numerical variables. 

```{r}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,svalm2,geymm2))
cor(X)
```

None of the variables are highly correlated. 


### Modeling the data


When exploring the data we realised that the $\texttt{svfn}$ variable and the two $\texttt{matssvaedi}$ variables are linearly dependent since they are describing the same thing. For the regression part we decided to use the $\texttt{matssvaedi}$ variables and for the classification we are going to use the $\texttt{svfn}$ variable. 

```{r}
dat = data
datN = dataOriginal
svfn = dat$svfn
svfnN = datN$svfn
dat$svfn = NULL
datN$svfn = NULL
```

Now we'll split the data into a training set with 80% of the data and a test set. 

```{r}
set.seed(5040)
n = dim(dat)[1]
train = sample(n,floor(n*0.8)) 
dat.train = dat[train,]
dat.test = dat[-train,]
datN.train = datN[train,]
datN.test = datN[-train,]

```

## Linear Regression

To start we are using linear regression to predict (the log of) $\texttt{nuvirdi}$ 

```{r}
fit.lm = lm(nuvirdi~.,dat.train)
summary(fit.lm)
```

All of the numerical variables are significant and most of the factors have significant levels, apart from $\texttt{fjbkar}$ and $\texttt{fjklos}$.

```{r}
fitN.lm = lm(nuvirdi~.,datN.train)
summary(fitN.lm)
```

We can see that Adjusted R-squared is lower for $\texttt{Nuvirdi}$ not on log-scale. 


```{r}
pred.lm = predict(fit.lm, dat.test)
RssLin = mean((exp(dat.test$nuvirdi)-exp(pred.lm))^2)
RssLin

predN.lm = predict(fitN.lm, datN.test)
RssLinN = mean((datN.test$nuvirdi - predN.lm)^2)
RssLinN
```

# Diagnostics of linear regression

Here below we can see some plots for the linear regression model(Log scaled).

```{r}
diagnost = fortify(fit.lm)
diagnost$.index = 1:(dim(diagnost)[1])
res_sd = sd(diagnost$.resid)
resPlot = ggplot(diagnost, aes(x=seq(1:length(.resid)),y=.resid)) +
                geom_point()+
                geom_hline(yintercept=0, col="red", linetype="dashed")+
                geom_hline(yintercept=2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=-2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=3*res_sd, col="green", linetype="dashed")+
                geom_hline(yintercept=-3*res_sd, col="green", linetype="dashed")+
                xlab("Index")+ylab("Residuals")+labs(title = "Index plot of residuals")+
                geom_text(aes(label=ifelse(abs(.resid)>3*res_sd,.index,"")),
                      hjust=-.5, vjust=0.4)+
                theme_grey()
resPlot
```

Here we can see the residuals plotted against their index. The blue line is 2 times the standard deviation of the residuals and the green line is 3 times the standard deviation. We can see that there are few points that exceed the green line and they could be looked at more closely. Also we can see that there seems to be a constant variance.

```{r}
QQPlot<-ggplot(diagnost, aes(sample = .resid))+
        stat_qq()+
        geom_abline(slope=res_sd)+
        ylab("Residuals")+
        theme_grey()
QQPlot
```

Here we can see the normal Q-Q plot for the model. It looks good and they seem to follow a normal distribution but the ends are more spread out they are heavy tailed.

```{r}
LevPlot <- ggplot(diagnost, aes(x=seq(1:length(.hat)),y=.hat))+
                  geom_point()+
                  geom_hline(yintercept=4*mean(diagnost$.hat), col="red", linetype="dashed")+
                  xlab("Index")+
                  ylab("Leverages")+
                  geom_text(aes(label=ifelse(.hat > 4*mean(diagnost$.hat),.index,"")),
                            hjust=0, vjust=0)+
                  theme_grey()
LevPlot
```

Here we can see the Leverage plot. There seems to be few points that have very high leverage. These point should be looked at more closely to examine weather they are outliers or not.


## Lasso Regression

Now we use Lasso regression to see if it improves the linear regression.

```{r,eval = F}
ytrain = data.matrix(dat.train[,"nuvirdi"])
Xtrain = data.matrix(dat.train[,!(colnames(dat.train) %in% c("nuvirdi"))])
Xtest = data.matrix(dat.test[,!(colnames(dat.train) %in% c("nuvirdi"))])
grid = 10^seq(1, -20, length=1000)
fit.lasso = cv.glmnet(Xtrain,ytrain,alpha=1, lambda=grid, thresh=1e-12)
plot(fit.lasso)
```

From the plot above it seems that the best lambda has no zero predictors.


```{r,eval = F}
best.lambda = fit.lasso$lambda.min
pred.lasso = predict(fit.lasso,Xtest,s=best.lambda)
RssLasso = mean((exp(dat.test$nuvirdi)-exp(pred.lasso))^2)
RssLasso

```

Here we can see that when we do Lasso regression the MSE is much higher then for a normal linear regression.


```{r,eval = F}
best.model = glmnet(Xtrain, ytrain, alpha = 1)
predict(best.model, s = fit.lasso$lambda.1se, type = "coefficients")
```

The first two predictors that tend to zero are $\texttt{fjeld}$ and $\texttt{svalm2}$.


Now we fit a Lasso model on the normal data.

```{r}
ytrainN = data.matrix(datN.train[,"nuvirdi"])
XtrainN = data.matrix(datN.train[,!(colnames(dat.train) %in% c("nuvirdi"))])
XtestN = data.matrix(datN.test[,!(colnames(dat.train) %in% c("nuvirdi"))])
fitN.lasso = cv.glmnet(XtrainN,ytrainN,alpha=1, lambda=grid, thresh=1e-12)

bestN.lambda = fitN.lasso$lambda.min
predN.lasso = predict(fitN.lasso,XtestN,s=bestN.lambda)
RssLassoN = mean((datN.test$nuvirdi - predN.lasso)^2)
RssLassoN
```



##Boosting

Here we perform our second method in regression to predict $\texttt{nuvirdi}$

```{r,eval = F}
library(gbm)
set.seed(5040)
lambd <-  seq(0.001, 0.02, by=0.001)
lambd <- c(lambd, seq(0.02, 0.5, by=0.01))
m <- length(lambd)
testErr <- rep(NA, m)
testErrN <- rep(NA, m)
for (i in 1:m) {
    boostCol = gbm(nuvirdi ~ ., data = dat.train,
                    distribution = "gaussian",
                    n.trees = 200, 
                    shrinkage = lambd[i])
    testPred = predict(boostCol, dat.test, n.trees = 200)
    testErr[i] = mean((dat.test$nuvirdi - testPred)^2)
    boostColN = gbm(nuvirdi ~ ., data = datN.train,
                    distribution = "gaussian",
                    n.trees = 200, 
                    shrinkage = lambd[i])
    testPredN = predict(boostColN, datN.test, n.trees = 200)
    testErrN[i] = mean((datN.test$nuvirdi - testPredN)^2)
    print(i)
}

bestLam = lambd[which.min(testErr)]
bestLamN = lambd[which.min(testErrN)]

boostPlot =ggplot(data.frame(x=lambd, y=testErr), aes(x=x, y=y)) +
            xlab("Shrinkage") + 
            ylab("Test MSE") + 
            geom_point()

boostPlot2 = ggplot(data.frame(x=lambd, y=testErrN), aes(x=x, y=y)) +
              xlab("Shrinkage") + 
              ylab("Test MSE(Normal)") + 
              geom_point()

grid.arrange(boostPlot,
             boostPlot2,ncol=2)

```

From the plot above we are trying to find the best shrinkage parameter lambda. We get that it's $\lambda=0.27$ for the transformed data and $\lambda=0.22$ for the normal data. Now we fit a boosting model using 1000 trees with these $\lambda$.

```{r,eval = F}
set.seed(5040)
boost.fit <- gbm(nuvirdi ~ ., data = dat.train, distribution = "gaussian",
                 n.trees = 1000, shrinkage =bestLam )
boost.pred <- predict(boost.fit, dat.test, n.trees = 1000)
RssBoost = mean((exp(dat.test$nuvirdi) - exp(boost.pred))^2)

RssBoost



boostN.fit <- gbm(nuvirdi ~ ., data = datN.train, distribution = "gaussian",
                  n.trees = 1000, shrinkage =bestLamN )
boostN.pred <- predict(boostN.fit, datN.test, n.trees = 1000)
RssBoostN = mean((datN.test$nuvirdi - boostN.pred)^2)

RssBoostN
```


```{r}
library(knitr)
linear_summary <- c(RssLin,RssLinN) 
lasso_summary <- c(RssLasso,RssLassoN) 
boost_summary <- c(RssBoost,RssBoostN) 

table <- rbind(linear_summary,lasso_summary,boost_summary)
rownames(table) <- c("Linear","Lasso", "Boosting") 
colnames(table) <- c("MSE (Log)","MSE Normal")
kable(round(table,6),caption = "Summary of MSE from the methods")
```

From the table above we can see that Boosting and linear regression on the transformed data are doing the best job in predicting nuvirdi.

### Classification 

Here we are going to predict in which town(Svfn) the properties belong to. We start by taking only properties from Reykjavik, Kopavogi and Hafnarfirði.
The classes are 
Rvk
Kop
Hfj

```{r}
dat = data.frame(dat,svfn)
mats = dat$matssvaedi
undir = dat$undirmatssvaedi
dat$matssvaedi = NULL
dat$undirmatssvaedi = NULL

area = levels(data$svfn)[c(1,2,5)]
dat = dat[which(data$svfn %in% area),]
dat$svfn = droplevels(dat$svfn)
```

```{r}
datN = data.frame(datN,svfn)
mats = datN$matssvaedi
undir = datN$undirmatssvaedi
datN$matssvaedi = NULL
datN$undirmatssvaedi = NULL

datN = datN[which(data$svfn %in% area),]
datN$svfn = droplevels(datN$svfn)
```

Here we take out Matssvæði and undirmatsvæði because they are describing the same thing as Svfn.

```{r}
set.seed(5040)
n = dim(dat)[1]
p = dim(dat)[2]
train = sample(n,floor(n*0.8))


dat.train = dat[train,]
dat.test = dat[-train,]
datN.train=datN[train,]
datN.test = datN[-train,]

n2 = dim(dat.train)[1]
val = sample(n2,floor(n2*0.8))
dat.val = dat.train[-val,]
dat.train = dat.train[val,]
datN.val = datN.train[-val,]
datN.train = datN.train[val,]
```

We also split to a train set, test set and a validation set to use in cross validation for SVM.


```{r}
table(dat$svfn)
```

We can see that most of the properties are in Reykjavik.

## Random Forest

First we use Random Forest to try and classify into these three groups.

We start by finding the number m which is the sample of random predictors that is chosen as split candidates in each split.

```{r}
library(randomForest)
set.seed(5040)
err = integer()
errN = integer()
mtrys = seq(from = 2, to = 20, by = 1)
for(i in mtrys){
  fit.rf = randomForest(svfn~., dat.train, mtry = i, ntree = 200) 
  pred.rf = predict(fit.rf, dat.val)
  err[i-1] = mean(dat.val$svfn != pred.rf)
  fitN.rf = randomForest(svfn~., datN.train, mtry = i, ntree = 200)
  predN.rf = predict(fitN.rf, datN.val)
  errN[i-1] = mean(datN.val$svfn != predN.rf)  
  print(i)
}

RFlog = ggplot(data=data.frame(x=mtrys,y=err),aes(x=x,y=y))+
        xlab("nr of variables considered at each split")+
        ylab("Misclassification Error(Log)")+
        geom_point()

RFNor = ggplot(data=data.frame(x=mtrys,y=errN),aes(x=x,y=y))+
        xlab("nr of variables considered at each split")+
        ylab("Misclassification Error(Normal)")+
        geom_point()


grid.arrange(RFlog,RFNor,ncol=2)
```

Here we can see the corresponding Missclassification Error for different values of m.


```{r}
minErr = which.min(err)
fit.rf = randomForest(svfn~., dat.train, mtry =minErr+1, ntree = 200)
pred.rf = predict(fit.rf, dat.test)
misErr = mean(dat.test$svfn != pred.rf)
misErr

minErrN = which.min(errN)
fitN.rf = randomForest(svfn~., datN.train, mtry =minErrN+1, ntree = 200)
predN.rf = predict(fitN.rf, datN.test)
misErrN = mean(datN.test$svfn != predN.rf)

misErrN

```



```{r}
fit.rf
importance(fit.rf)
```

Here we can see both the confusion matrix and the most important variables for the classification.

The most important variables are:
Aldur
Ummal
Ibm2
Nuvirdi



## Support Vector Machine

Here we use SVM without a kernel and with radial kernel to try and classify. The svm function from the e1071 library uses the One-versus-one classification. We start by using the validation set to choose C and then $\gamma$ for the radial kernal. We also scale the data for the SVM because it gave a better result.


```{r}
library(e1071)
set.seed(5040)

tune.svm = tune(svm, svfn~.,
                data = dat.val,
                scale=T, 
                ranges = list(cost = c(1,10,50,100,200,300)))  
summary(tune.svm)


tuneN.svm = tune(svm, svfn~.,
                data = datN.val,
                scale=T, 
                ranges = list(cost = c(1,10,50,100,200,300)))  
summary(tuneN.svm)

```

Here we can see that $C=300$ gives the best result for both data sets. Let's fit a model with $C=300$ on the training set.

```{r}
fit.svm = svm(svfn~.,
              data = dat.train,
              scale = T,
              cost = 300)
pred.svm = predict(fit.svm,dat.test)
err.svm = mean(pred.svm!=dat.test$svfn)
err.svm


fitN.svm = svm(svfn~.,
              data = datN.train,
              scale = T,
              cost = 100)
predN.svm = predict(fitN.svm,datN.test)
errN.svm = mean(predN.svm!=datN.test$svfn)
errN.svm
```


Below is the confusion matrix for the transformed data.

```{r}
table(pred.svm,dat.test$svfn)
```
 
 
Here we fit a model with radial kernel. Let's use the validation set to choose the parameters.

```{r}
set.seed(5040)
tune.svm2 = tune(svm, svfn~.,
                data = dat.val,
                scale=T,
                kernel="radial",
                ranges = list(gamma=c(0.1,0.5),cost = c(50,100,200,300))) 
summary(tune.svm2)

tuneN.svm2 = tune(svm, svfn~.,
                data = datN.val,
                scale=T,
                kernel="radial",
                ranges = list(gamma=c(0.1,0.5),cost = c(50,100,200,300))) 
summary(tuneN.svm2)

```

Here we can see that $\gamma=0.1$ and $C=300$ gives the best result for both transformed data and normal. Let's fit a model with these parameters.

```{r}
set.seed(5040)
fit.svm2 = svm(svfn~.,
              data = dat.train,
              scale = T,
              kernel="radial",
              gamma = 0.1,
              cost = 300)
pred.svm2 = predict(fit.svm2,dat.test)
err.svm2 = mean(pred.svm2!=dat.test$svfn)
err.svm2

fitN.svm2 = svm(svfn~.,
              data = datN.train,
              scale = T,
              kernel="radial",
              gamma = 0.1,
              cost = 300)
predN.svm2 = predict(fitN.svm2,datN.test)
errN.svm2 = mean(predN.svm2!=datN.test$svfn)
errN.svm2
```

Below we can see the confusion matrix

```{r}
table(pred.svm2,dat.test$svfn)
```

Here is the summary of the classification methods. Random Forest is doing a much better job then SVM. It seems that SVM is not doing a good job for this data set.

```{r}
Random_summary <- c(misErr,misErrN) 
WithoutSVM_summary <- c(err.svm,errN.svm) 
WithSVM_summary <- c(err.svm2,errN.svm2) 

table <- rbind(Random_summary,WithoutSVM_summary,WithSVM_summary)
rownames(table) <- c("Random Forest","SVM without Kernel", "SVM with radial") 
colnames(table) <- c("Missclassification error(Log)","Misclassification error(Normal)")
kable(round(table,6),caption = "Summary of MisClas error from the methods")
```


### Appendix

###Data preprocessing

```{r}
data = read.csv("gagnasafn_ibudarhusnaedi.csv", header = T, sep = ";", dec = ",")
```

We start by removing all the data which is not in the capital region. 

```{r, eval = F}
capitalArea = c(0000,1000,1100,1300,1400,1604,1606)
data = data[data$svfn %in% capitalArea,]
```

Now we are going to remove the variables that we won't be using.

```{r, eval = F}
delete = names(data)[c(1,2,5,6,7,10,11,12,13,14,15,17,18,19,20,21,
                       25,26,30,37,38,40,41,42,44,47,50,52,53)]
data = data[,-which(names(data) %in% delete)]
```

Some variable have many levels and few observations on some levels so we are going to remove those observations. 

```{r, eval = F}
lev = levels(data$teg_eign)[c(1,2,7,8,11)] 
data = data[which(data$teg_eign %in% lev),]
data$teg_eign = droplevels(data$teg_eign)

data$matssvaedi = as.factor(data$matssvaedi)
lev = levels(data$matssvaedi)[c(61,62)]
data = data[-which(data$matssvaedi %in% lev),]
data$matssvaedi = droplevels(data$matssvaedi)

data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
lev = levels(data$undirmatssvaedi)[c(10,36,38,39,50,55)]
data = data[-which(data$undirmatssvaedi %in% lev),]
data$undirmatssvaedi = droplevels(data$undirmatssvaedi)
```

Many of the numerical variables should be factors, and some factors should be numerical. We are also going to combine some of the levels for certain factors since the higher levels have very few observations in many cases.

```{r, eval = F}
data$svfn = as.factor(data$svfn)
data$lyfta[which(data$lyfta != 0)] = 1
data$lyfta = as.factor(data$lyfta)
data$fjsturt[which(!(data$fjsturt %in% c(0,1)))] = 2
data$fjsturt = as.factor(data$fjsturt)
data$fjklos[which(!(data$fjklos %in% c(0,1)))] = 2
data$fjklos = as.factor(data$fjklos)
data$fjbkar[which(!(data$fjbkar %in% c(0,1)))] = 2
data$fjbkar = as.factor(data$fjbkar)
data$fjeld[which(!(data$fjeld %in% c(0,1)))] = 2
data$fjeld = as.factor(data$fjeld)
data$fjstof[which(!(data$fjstof %in% c(0,1)))] = 2
data$fjstof = as.factor(data$fjstof)
data$fjhaed = as.factor(data$fjhaed)
data$fjherb[which(!(data$fjherb %in% c(0,1,2,3,4,5)))] = 6
data$fjherb = as.factor(data$fjherb)
data$svalm2 = as.numeric(data$svalm2)
data$fjmib = as.factor(data$fjmib)
data$ibteg = as.factor(data$ibteg)
data$svfn = revalue(data$svfn, c("0"="Rvk","1000"="Kop",
                                 "1100"="Sel","1300"="Grb",
                                 "1400"="Hfj","1604"="Mos"))
data$ibteg = revalue(data$ibteg, c("11"="1","12"="2"))
data$byggar = droplevels(data$byggar)
```

We are going to change the kdagur variable so that we have the days since the property was bought. 

```{r, eval = F}
data$kdagur <- strptime(x = as.character(data$kdagur),
                                format = "%d.%m.%Y")
mydate <- as.Date("2018-04-01")
data$kdagur <- as.Date(data$kdagur)
data$kdagur <- as.numeric(mydate-data$kdagur)
```

And we're going to change the byggar variable into the age of the house in years. 

```{r, eval = F}
data$byggar <- 2018 - as.numeric(as.character(data$byggar))
colnames(data)[5] = "aldur"
```

Now we remove 0 values for nuvirdi and check for NA values. 
```{r, eval = F}
noNuvirdi <- which(data$nuvirdi==0)
data = data[-noNuvirdi,]
dim(data) == dim(na.omit(data))
```

Now we'll check whether any of the numerical variables are highly correlated. 

```{r, eval = F}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,ntm2,svalm2,geymm2))
cor(X)
```

The $\texttt{ibm2}$ and $\texttt{ntm2}$  have a very high correlation so we removed one of them. 

```{r, eval = F}
data$ntm2 = NULL 
```

Now the data is ready for visualization. 
