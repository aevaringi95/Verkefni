---
title: "Final Project"
author: "Rakel María Brynjólfsdóttir og Ævar Ingi Jóhannesson"
date: "March 23, 2018"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
options(scipen=999)
```

```{r, warning = F, message = F}
library(plyr); library(dplyr); library(lubridate)
library(ggplot2)
library(grid)
library(gridExtra)
library(glmnet)
library(e1071)
library(randomForest)
library(knitr)
library(gbm)
```

We start by loading the data which has already been cleaned. The particulars of the data preproccesing can be seen in the appendix. 

```{r}
data = read.csv("MyData.csv")
```

```{r, echo = F}
data$fjmib = as.factor(data$fjmib)
data$lyfta = as.factor(data$lyfta)
data$fjhaed = as.factor(data$fjhaed)
data$fjbkar = as.factor(data$fjbkar)
data$fjsturt = as.factor(data$fjsturt)
data$fjklos = as.factor(data$fjklos)
data$fjeld = as.factor(data$fjeld)
data$fjherb = as.factor(data$fjherb)
data$fjstof = as.factor(data$fjstof)
data$matssvaedi = as.factor(data$matssvaedi)
data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
data$ibteg = as.factor(data$ibteg)
dataOriginal = data
```

The data is now as follows: 

  Variable     |  type     | Description               |  levels
-------------- | --------- | ------------------------- | ----------------------- 
kdagur         | numerical | days since last bought    | 
nuvirdi        | numerical | log of current price      | 
teg_eign       | factor    | type of property          | Einbýlishús, Fjölbýlishús, Íbúðareign, Íbúðarhús, Raðhús
svfn           | factor    | town id                   | Grb, Hfj, Kop, Mos, Rvk, Sel
aldur          | numerical | age of house              |   
fjmib          | factor    | nr of apartments          | 1,2,3,4
lyfta          | factor    | elevator or not           | 0,1
ummal          | numerical | circumference of property | 
ibm2           | numerical | area of property          |
ntm2           | numerical | netto area of property    |
fjhaed         | factor    | number of floors          | 1,2,3,4
fjbkar         | factor    | number of bathtubs        | 0,1,2 (2 or more)
fjsturt        | factor    | number of showers         | 0,1,2 (2 or more)
fjklos         | factor    | number of toilets         | 0,1,2 (2 or more)
fjeld          | factor    | number of kitchens        | 0,1,2 (2 or more)
fjherb         | factor    | number of rooms           | 0,1,2,3,4,5,6 (6 or more)
fjstof         | factor    | number of livingrooms     | 0,1,2 (2 or more)
stig10         | numerical | construction stage        |
bilskurm2      | numerical | area of garage            |
svalm2         | numerical | area of balcony           |
geymm2         | numerical | area of storage space     |
matssvaedi     | factor    | location                  | 60 levels
undirmatssvaedi| factor    | sub-location              | 62 levels
ibteg          | factor    | type of property          | 1(single-family), 2(aparment building)

### Descriptive plots of the data

We start by looking at the density of the $\texttt{nuvirdi}$ variable.  

```{r}
density1 <- ggplot(data,aes(nuvirdi))+
  geom_density()

density2 <- ggplot(data,aes(log(nuvirdi)))+
  geom_density()

grid.arrange(density1,density2,nrow=2)
```

It's evident that when we plot $\texttt{nuvirdi}$ on a log scale it looks a lot more like the normal distribution, so we're going to log transform the response. 

```{r}
data$nuvirdi = log(data$nuvirdi)
```

Let's plot $\texttt{nuvirdi}$ against some variables to see if there are any relationships. 

```{r}
p1 <- ggplot(data, aes(x=kdagur,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey()

p2 <- ggplot(data, aes(x=stig10,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p3 <- ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p4 <- ggplot(data, aes(x=ummal,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p5 <- ggplot(data, aes(x=aldur,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p6 <- ggplot(data, aes(x=bilskurm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p7 <- ggplot(data, aes(x=svalm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p8 <- ggplot(data, aes(x=ibm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

grid.arrange(p1,p2,p3,p4,nrow = 2)
grid.arrange(p5,p6,p7,p8,nrow = 2)
```

It looks like $\texttt{geymm2}$, $\texttt{bilskurm2}$ and $\texttt{ibm2}$ might have a more linear relatioship with nuvirdi on a log scale. 

```{r}
B = data$bilskurm2
ind1 = which(B == 0)
B[-ind1] = log(B[-ind1])
I = log(data$ibm2)
G = data$geymm2
ind3 = which(G == 0)
G[-ind3] = log(G[-ind3])
```

```{r}
p1 <- ggplot(data, aes(x=B,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey() + labs(x="bilskurm2")

p2 <- ggplot(data, aes(x=I,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="ibm2")

p3 <- ggplot(data, aes(x=G,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="geymm2")

grid.arrange(p1,p2,p3,nrow = 2)
```

A log transformation is not suitable for the $\texttt{bilskurm2}$ variable, but it definitely is for $\texttt{ibm2}$ and $\texttt{geymm2}$, although we will have to shift the values so that they are positive. 

```{r}
data$ibm2 = log(data$ibm)
G[-ind3] = G[-ind3] + 1.7
data$geymm2 = G
ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() 
```


Now we'll make some box plots of the factor variables. 

```{r}
f1 <- ggplot(data,aes(x=teg_eign,y=nuvirdi))+
          geom_boxplot()

f2 <- ggplot(data,aes(x=svfn,y=nuvirdi))+
          geom_boxplot()

f3 <- ggplot(data,aes(x=fjmib,y=nuvirdi))+
          geom_boxplot()

f4 <- ggplot(data,aes(x=lyfta,y=nuvirdi))+
          geom_boxplot()

f5 <- ggplot(data, aes(x=fjhaed,y=nuvirdi)) +
          geom_boxplot()

f6 <- ggplot(data,aes(x=fjbkar,y=nuvirdi))+
          geom_boxplot()

f7 <- ggplot(data,aes(x=fjsturt,y=nuvirdi))+
          geom_boxplot()

f8 <- ggplot(data,aes(x=fjklos,y=nuvirdi))+
          geom_boxplot()

f9 <- ggplot(data,aes(x=fjeld,y=nuvirdi))+
          geom_boxplot()

f10 <- ggplot(data,aes(x=fjherb,y=nuvirdi))+
          geom_boxplot()

f11 <- ggplot(data, aes(x=fjstof,y=nuvirdi)) +
          geom_boxplot()

f12 <- ggplot(data, aes(x=ibteg,y=nuvirdi)) +
          geom_boxplot()

grid.arrange(f1,f2,f3,
             f4,f5,
             f6,f7,
             f8,f9,f10,f11,f12,ncol=3)
```

Most of the factor variables seem to have a relationship with $\texttt{nuvirdi}$, especially $\texttt{fjmib}$, $\texttt{fjhaed}$ and $\texttt{fjherb}$. 

Finally, we'll check the correlation between the numerical variables. 

```{r}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,svalm2,geymm2))
cor(X)
```

None of the variables are highly correlated. 


### Modeling the data


When exploring the data we realised that the $\texttt{svfn}$ variable and the two $\texttt{matssvaedi}$ variables are linearly dependent since they are describing the same thing. For the regression part we decided to use the $\texttt{matssvaedi}$ variables and for the classification we are going to use the $\texttt{svfn}$ variable. We will evaluate both data sets, transformed(Log scaled) and normal.

```{r}
dat = data
datN = dataOriginal
svfn = dat$svfn
svfnN = datN$svfn
dat$svfn = NULL
datN$svfn = NULL
```

Now we'll split the data into a training set with 80% of the data and a test set. 

```{r}
set.seed(5040)
n = dim(dat)[1]
train = sample(n,floor(n*0.8)) 
dat.train = dat[train,]
dat.test = dat[-train,]
datN.train = datN[train,]
datN.test = datN[-train,]

```

## Linear Regression

To start we are using linear regression to predict (the log of) $\texttt{nuvirdi}$ 

```{r}
fit.lm = lm(nuvirdi~.,dat.train)
summary(fit.lm)
```

All of the numerical variables are significant and most of the factors have significant levels, apart from $\texttt{fjbkar}$ and $\texttt{fjeld}$. The $R^2$-adjusted for this model is $0.9086$.

Now we'll do the same for the non-transformed data. 

```{r}
fitN.lm = lm(nuvirdi~.,datN.train)
summary(fitN.lm)
```

Now all the variables are significant, however the $R^2$-adjusted as dropped to $0.8598$. 

Finally we will calculated the MSE for the two models. 

```{r}
pred.lm = predict(fit.lm, dat.test)
RssLin = mean((exp(dat.test$nuvirdi)-exp(pred.lm))^2)

predN.lm = predict(fitN.lm, datN.test)
RssLinN = mean((datN.test$nuvirdi - predN.lm)^2)
```

The MSE for the first model is `r RssLin` and `r RssLinN` for the second model. Therefore it's clear that a log transformation is suitable when performing linear regression on this data.

# Diagnostics of linear regression

Here below we can see some plots for the linear regression model (log scaled).

```{r}
diagnost = fortify(fit.lm)
diagnost$.index = 1:(dim(diagnost)[1])
res_sd = sd(diagnost$.resid)
resPlot = ggplot(diagnost, aes(x=seq(1:length(.resid)),y=.resid)) +
                geom_point()+
                geom_hline(yintercept=0, col="red", linetype="dashed")+
                geom_hline(yintercept=2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=-2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=3*res_sd, col="green", linetype="dashed")+
                geom_hline(yintercept=-3*res_sd, col="green", linetype="dashed")+
                xlab("Index")+ylab("Residuals")+labs(title = "Index plot of residuals")+
                geom_text(aes(label=ifelse(abs(.resid)>3*res_sd,.index,"")),
                      hjust=-.5, vjust=0.4)+
                theme_grey()
resPlot
```

Here we can see the residuals plotted against their index. The blue line is 2 times the standard deviation of the residuals and the green line is 3 times the standard deviation. We can see that there are few points that exceed the green line and they could be looked at more closely. Also we can see that there seems to be a constant variance.

```{r}
QQPlot<-ggplot(diagnost, aes(sample = .resid))+
        stat_qq()+
        geom_abline(slope=res_sd)+
        ylab("Residuals")+
        theme_grey()
QQPlot
```

Here we can see the normal Q-Q plot for the model. It looks good and they seem to follow a normal distribution but the ends are quite spread out so the distribution is heavy tailed. 

```{r}
LevPlot <- ggplot(diagnost, aes(x=seq(1:length(.hat)),y=.hat))+
                  geom_point()+
                  geom_hline(yintercept=4*mean(diagnost$.hat), col="red", linetype="dashed")+
                  xlab("Index")+
                  ylab("Leverages")+
                  geom_text(aes(label=ifelse(.hat > 4*mean(diagnost$.hat),.index,"")),
                            hjust=0, vjust=0)+
                  theme_grey()
LevPlot
```

Here we can see the Leverage plot. There seems to be few points that have very high leverage. These points should be looked at more closely to examine weather they are outliers or not.


## Lasso Regression

Now we use Lasso regression to see if it improves the linear regression.

```{r,eval = T}
set.seed(5040)
ytrain = data.matrix(dat.train[,"nuvirdi"])
Xtrain = data.matrix(dat.train[,!(colnames(dat.train) %in% c("nuvirdi"))])
Xtest = data.matrix(dat.test[,!(colnames(dat.train) %in% c("nuvirdi"))])
grid = 10^seq(1, -20, length=1000)
fit.lasso = cv.glmnet(Xtrain,ytrain,alpha=1, lambda=grid, thresh=1e-12)
plot(fit.lasso)
```

From the plot above it seems that the best lambda has no zero predictors.


```{r,eval = T}
best.lambda = fit.lasso$lambda.min
pred.lasso = predict(fit.lasso,Xtest,s=best.lambda)
RssLasso = mean((exp(dat.test$nuvirdi)-exp(pred.lasso))^2)
```

The best $\lambda$ is `r best.lambda` which is very small so there is not much penalty on the size of coefficients. The corresponding MSE is `r RssLasso` which is much higher than we got for linear regression. 


```{r,eval = T}
best.model = glmnet(Xtrain, ytrain, alpha = 1)
predict(best.model, s = fit.lasso$lambda.1se, type = "coefficients")
```

The first three predictors that tend to zero are $\texttt{fjeld}$, $\texttt{fjstof}$ and $\texttt{svalm2}$.


Now we fit a Lasso model on the normal data.

```{r}
set.seed(5040)
ytrainN = data.matrix(datN.train[,"nuvirdi"])
XtrainN = data.matrix(datN.train[,!(colnames(dat.train) %in% c("nuvirdi"))])
XtestN = data.matrix(datN.test[,!(colnames(dat.train) %in% c("nuvirdi"))])
fitN.lasso = cv.glmnet(XtrainN,ytrainN,alpha=1, lambda=grid, thresh=1e-12)

bestN.lambda = fitN.lasso$lambda.min
predN.lasso = predict(fitN.lasso,XtestN,s=bestN.lambda)
RssLassoN = mean((datN.test$nuvirdi - predN.lasso)^2)
```

Here we get $\lambda =$ `r bestN.lambda` and the MSE for this model is `r RssLassoN` so a little higher than for the log-scaled data. 


##Boosting

Here we perform our final method in regression to predict $\texttt{nuvirdi}$. We start by checking which values of lambda gives us the best error. We do this simultaneously for the tranformed and untransformed data.  

```{r,eval = T}
set.seed(5040)
lambd <-  seq(0.001, 0.02, by=0.001)
lambd <- c(lambd, seq(0.02, 0.5, by=0.01))
m <- length(lambd)
testErr <- rep(NA, m)
testErrN <- rep(NA, m)
for (i in 1:m) {
    boostCol = gbm(nuvirdi ~ ., data = dat.train,
                    distribution = "gaussian",
                    n.trees = 200, 
                    shrinkage = lambd[i])
    testPred = predict(boostCol, dat.test, n.trees = 200)
    testErr[i] = mean((exp(dat.test$nuvirdi) - exp(testPred))^2)
    boostColN = gbm(nuvirdi ~ ., data = datN.train,
                    distribution = "gaussian",
                    n.trees = 200, 
                    shrinkage = lambd[i])
    testPredN = predict(boostColN, datN.test, n.trees = 200)
    testErrN[i] = mean((datN.test$nuvirdi - testPredN)^2)
}

bestLam = lambd[which.min(testErr)]
bestLamN = lambd[which.min(testErrN)]

boostPlot =ggplot(data.frame(x=lambd, y=testErr), aes(x=x, y=y)) +
            xlab("Shrinkage") + 
            ylab("Test MSE") + 
            geom_point()

boostPlot2 = ggplot(data.frame(x=lambd, y=testErrN), aes(x=x, y=y)) +
              xlab("Shrinkage") + 
              ylab("Test MSE(Normal)") + 
              geom_point()

grid.arrange(boostPlot,
             boostPlot2,ncol=2)
```


From the plot above we can see how the MSE decreases as a function of $\lambda$ and then increases slightly again. We get that the lowest $\lambda$ for the transformed data it's $\lambda=0.33$. For the normal data it's $\lambda=0.15$. Now we fit a boosting model using 1000 trees with these $\lambda$.

```{r,eval = T}
set.seed(5040)
boost.fit <- gbm(nuvirdi ~ ., data = dat.train, distribution = "gaussian",
                 n.trees = 1000, shrinkage =bestLam )
boost.pred <- predict(boost.fit, dat.test, n.trees = 1000)
RssBoost = mean((exp(dat.test$nuvirdi) - exp(boost.pred))^2)
```

The MSE for the Boosting model is `r RssBoost` which is the lowest we've seen so far. 

Now we repeat this on the untransformed data. 

```{r}
boostN.fit <- gbm(nuvirdi ~ ., data = datN.train, distribution = "gaussian",
                  n.trees = 1000, shrinkage =bestLamN )
boostN.pred <- predict(boostN.fit, datN.test, n.trees = 1000)
RssBoostN = mean((datN.test$nuvirdi - boostN.pred)^2)
```

Now we get en MSe of `r RssBoostN` which is a lot higher than for the transformed data. 


```{r}
linear_summary <- c(RssLin,RssLinN) 
lasso_summary <- c(RssLasso,RssLassoN) 
boost_summary <- c(RssBoost,RssBoostN) 

table <- rbind(linear_summary,lasso_summary,boost_summary)
rownames(table) <- c("Linear","Lasso", "Boosting") 
colnames(table) <- c("MSE (Log)","MSE Normal")
kable(round(table,6),caption = "Summary of MSE from the methods")
```

From the table above we can see that Boosting and linear regression on the transformed data are doing the best job in predicting $\texttt{nuvirdi}$

### Classification 

Here we are going to predict in which town ($\texttt{svfn}$) the properties belong are. We start by taking only properties from Reykjavik, Kopavogi and Hafnarfirði.

So the classes are 

- Rvk
- Kop
- Hfj

Now we add the $\texttt{svfn}$ variable back to the data frame and remove the $\texttt{matssvaedi}$ variables. 

```{r}
dat = data.frame(dat,svfn)
mats = dat$matssvaedi
undir = dat$undirmatssvaedi
dat$matssvaedi = NULL
dat$undirmatssvaedi = NULL

area = levels(data$svfn)[c(2,3,5)]
dat = dat[which(data$svfn %in% area),]
dat$svfn = droplevels(dat$svfn)
```

```{r}
datN = data.frame(datN,svfn)
mats = datN$matssvaedi
undir = datN$undirmatssvaedi
datN$matssvaedi = NULL
datN$undirmatssvaedi = NULL

datN = datN[which(data$svfn %in% area),]
datN$svfn = droplevels(datN$svfn)
```

```{r}
set.seed(5040)
n = dim(dat)[1]
p = dim(dat)[2]
train = sample(n,floor(n*0.8))

dat.train = dat[train,]
dat.test = dat[-train,]
datN.train=datN[train,]
datN.test = datN[-train,]

n2 = dim(dat.train)[1]
val = sample(n2,floor(n2*0.8))
dat.val = dat.train[-val,]
dat.train = dat.train[val,]
datN.val = datN.train[-val,]
datN.train = datN.train[val,]
```

We also split to a train set, test set and a validation set to use in cross validation for SVM.

```{r}
table(dat$svfn)
```

We can see that most of the properties are in Reykjavik.

## Random Forest

First we use Random Forest to try and classify into these three groups.

We start by finding the  tuning parameter $m$ which is the sample of random predictors that is chosen as split candidates in each split.

```{r}
set.seed(5040)
err = integer()
errN = integer()
mtrys = seq(from = 2, to = 20, by = 1)
for(i in mtrys){
  fit.rf = randomForest(svfn~., dat.train, mtry = i, ntree = 200) 
  pred.rf = predict(fit.rf, dat.val)
  err[i-1] = mean(dat.val$svfn != pred.rf)
  fitN.rf = randomForest(svfn~., datN.train, mtry = i, ntree = 200)
  predN.rf = predict(fitN.rf, datN.val)
  errN[i-1] = mean(datN.val$svfn != predN.rf)  
}

RFlog = ggplot(data=data.frame(x=mtrys,y=err),aes(x=x,y=y))+
        xlab("nr of variables considered at each split")+
        ylab("Misclassification Error(Log)")+
        geom_point()

RFNor = ggplot(data=data.frame(x=mtrys,y=errN),aes(x=x,y=y))+
        xlab("nr of variables considered at each split")+
        ylab("Misclassification Error(Normal)")+
        geom_point()


grid.arrange(RFlog,RFNor,ncol=2)
```

Here we can see the missclassification error for different values of $m$. 

```{r}
minErr = which.min(err)
minErrN = which.min(errN)
```

The lowest error for the log-scaled data is at $m =$ `r minErr` and at $m =$ `r minErrN` for the normal data. 

```{r}
fit.rf = randomForest(svfn~., dat.train, mtry =minErr+1, ntree = 200)
pred.rf = predict(fit.rf, dat.test)
misErr = mean(dat.test$svfn != pred.rf)
```

The misclassification error for this model is `r misErr`.

```{r}
fitN.rf = randomForest(svfn~., datN.train, mtry =minErrN+1, ntree = 200)
predN.rf = predict(fitN.rf, datN.test)
misErrN = mean(datN.test$svfn != predN.rf)
```

The error for the normal data is `r misErrN` which is almost identical to the first model. 

```{r}
fit.rf
```

Here we can see both the confusion matrix for the model on the transformed data. 

```{r}
importance(fit.rf)
```

The most important variables are $\texttt{aldur}$, $\texttt{ummal}$ and $\texttt{ibm2}$. 


## Support Vector Machine

Here we use SVM without a kernel and with a radial kernel to classify. The $\texttt{svm()}$ function from the $\texttt{e1071}$ library uses the One-versus-one classification. We start by using the validation set to choose C and then $\gamma$ for the radial kernel. We also scale the data for the SVM because it gives a better result.

# Without Kernel

```{r}
set.seed(5040)
tune.svm = tune(svm, svfn~.,
                data = dat.val,
                scale=T, 
                ranges = list(cost = c(1,10,50,100,200,300)))  

tuneN.svm = tune(svm, svfn~.,
                data = datN.val,
                scale=T, 
                ranges = list(cost = c(1,10,50,100,200,300)))  

summary(tune.svm)
summary(tuneN.svm)
```

Here we can see that $C=100$ gives the best result for both data sets. Let's fit a model with $C=100$ on the training set.

```{r}
fit.svm = svm(svfn~.,
              data = dat.train,
              scale = T,
              cost = 100)
pred.svm = predict(fit.svm,dat.test)
err.svm = mean(pred.svm!=dat.test$svfn)

fitN.svm = svm(svfn~.,
              data = datN.train,
              scale = T,
              cost = 100)
predN.svm = predict(fitN.svm,datN.test)
errN.svm = mean(predN.svm!=datN.test$svfn)
```

The misclassification error for the log-scaled data is `r err.svm` and `r errN.svm` for the normal data. 

Below is the confusion matrix for the transformed data.

```{r}
table(pred.svm,dat.test$svfn)
```
 
# Radial Kernel

Now we are going to use a radial kernel. Let's use the validation set to choose the parameters.

```{r}
set.seed(5040)
tune.svm2 = tune(svm, svfn~.,
                data = dat.val,
                scale=T,
                kernel="radial",
                ranges = list(gamma=c(0.05,0.1,0.5),cost = c(50,100,200,300))) 

tuneN.svm2 = tune(svm, svfn~.,
                data = datN.val,
                scale=T,
                kernel="radial",
                ranges = list(gamma=c(0.05,0.1,0.5),cost = c(50,100,200,300))) 

summary(tune.svm2)
summary(tuneN.svm2)
```

Here we can see that $\gamma=0.05$ and $C=50$ gives the best result for both transformed data and normal. Let's fit a model with these parameters.

```{r}
set.seed(5040)
fit.svm2 = svm(svfn~.,
              data = dat.train,
              scale = T,
              kernel="radial",
              gamma = 0.05,
              cost = 50)
pred.svm2 = predict(fit.svm2,dat.test)
err.svm2 = mean(pred.svm2!=dat.test$svfn)

fitN.svm2 = svm(svfn~.,
              data = datN.train,
              scale = T,
              kernel="radial",
              gamma = 0.05,
              cost = 50)
predN.svm2 = predict(fitN.svm2,datN.test)
errN.svm2 = mean(predN.svm2!=datN.test$svfn)
```

The error for the log-transformed data is `r err.svm2` and `r errN.svm2` for the normal data. 

We tried using different values for $C$ and $\gamma$ and actually got better results even though the $\tune()$ function found other values. Perhaps the validation set is too small. When we use $C= 300$ and $\gamma = 0.1$ we get the following results. 

```{r}
fit.svm3 = svm(svfn~.,
              data = dat.train,
              scale = T,
              kernel="radial",
              gamma = 0.1,
              cost = 300)
pred.svm3 = predict(fit.svm3,dat.test)
err.svm3 = mean(pred.svm3!=dat.test$svfn)

fitN.svm3 = svm(svfn~.,
              data = datN.train,
              scale = T,
              kernel="radial",
              gamma = 0.1,
              cost = 300)
predN.svm3 = predict(fitN.svm3,datN.test)
errN.svm3 = mean(predN.svm3!=datN.test$svfn)
err.svm3
errN.svm3
```

Now there errors are `r err.svm3` and `r errN.svm3` respectively which is better than when using $C$ and $\gamma$ from the $\texttt{tune()}$ function. 

Below we can see the confusion matrix

```{r}
table(pred.svm3,dat.test$svfn)
```

Here is the summary of the classification methods. Random Forest is doing a much better job then SVM. It seems that SVM is not doing a good job for this data set.

```{r}
Random_summary <- c(misErr,misErrN) 
WithoutSVM_summary <- c(err.svm,errN.svm) 
WithSVM_summary <- c(err.svm3,errN.svm3) 

table <- rbind(Random_summary,WithoutSVM_summary,WithSVM_summary)
rownames(table) <- c("Random Forest","SVM without Kernel", "SVM with radial") 
colnames(table) <- c("Missclassification error(Log)","Misclassification error(Normal)")
kable(round(table,6),caption = "Summary of MisClas error from the methods")
```

Random Forest has the lowest misclassification error by far for both the normal data and the transformed data. SVM with a radial kernel does slightly better than the one without a kernel. 

### Appendix

###Data preprocessing

```{r}
data = read.csv("gagnasafn_ibudarhusnaedi.csv", header = T, sep = ";", dec = ",")
```

We start by removing all the data which is not in the capital region. 

```{r, eval = F}
capitalArea = c(0000,1000,1100,1300,1400,1604,1606)
data = data[data$svfn %in% capitalArea,]
```

Now we are going to remove the variables that we won't be using.

```{r, eval = F}
delete = names(data)[c(1,2,5,6,7,10,11,12,13,14,15,17,18,19,20,21,
                       25,26,30,37,38,40,41,42,44,47,50,52,53)]
data = data[,-which(names(data) %in% delete)]
```

Some variable have many levels and few observations on some levels so we are going to remove those observations. 

```{r, eval = F}
lev = levels(data$teg_eign)[c(1,2,7,8,11)] 
data = data[which(data$teg_eign %in% lev),]
data$teg_eign = droplevels(data$teg_eign)

data$matssvaedi = as.factor(data$matssvaedi)
lev = levels(data$matssvaedi)[c(61,62)]
data = data[-which(data$matssvaedi %in% lev),]
data$matssvaedi = droplevels(data$matssvaedi)

data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
lev = levels(data$undirmatssvaedi)[c(10,36,38,39,50,55)]
data = data[-which(data$undirmatssvaedi %in% lev),]
data$undirmatssvaedi = droplevels(data$undirmatssvaedi)
```

Many of the numerical variables should be factors, and some factors should be numerical. We are also going to combine some of the levels for certain factors since the higher levels have very few observations in many cases.

```{r, eval = F}
data$svfn = as.factor(data$svfn)
data$lyfta[which(data$lyfta != 0)] = 1
data$lyfta = as.factor(data$lyfta)
data$fjsturt[which(!(data$fjsturt %in% c(0,1)))] = 2
data$fjsturt = as.factor(data$fjsturt)
data$fjklos[which(!(data$fjklos %in% c(0,1)))] = 2
data$fjklos = as.factor(data$fjklos)
data$fjbkar[which(!(data$fjbkar %in% c(0,1)))] = 2
data$fjbkar = as.factor(data$fjbkar)
data$fjeld[which(!(data$fjeld %in% c(0,1)))] = 2
data$fjeld = as.factor(data$fjeld)
data$fjstof[which(!(data$fjstof %in% c(0,1)))] = 2
data$fjstof = as.factor(data$fjstof)
data$fjhaed = as.factor(data$fjhaed)
data$fjherb[which(!(data$fjherb %in% c(0,1,2,3,4,5)))] = 6
data$fjherb = as.factor(data$fjherb)
data$svalm2 = as.numeric(data$svalm2)
data$fjmib = as.factor(data$fjmib)
data$ibteg = as.factor(data$ibteg)
data$svfn = revalue(data$svfn, c("0"="Rvk","1000"="Kop",
                                 "1100"="Sel","1300"="Grb",
                                 "1400"="Hfj","1604"="Mos"))
data$ibteg = revalue(data$ibteg, c("11"="1","12"="2"))
data$byggar = droplevels(data$byggar)
```

We are going to change the kdagur variable so that we have the days since the property was bought. 

```{r, eval = F}
data$kdagur <- strptime(x = as.character(data$kdagur),
                                format = "%d.%m.%Y")
mydate <- as.Date("2018-04-01")
data$kdagur <- as.Date(data$kdagur)
data$kdagur <- as.numeric(mydate-data$kdagur)
```

And we're going to change the byggar variable into the age of the house in years. 

```{r, eval = F}
data$byggar <- 2018 - as.numeric(as.character(data$byggar))
colnames(data)[5] = "aldur"
```

Now we remove 0 values for nuvirdi and check for NA values. 
```{r, eval = F}
noNuvirdi <- which(data$nuvirdi==0)
data = data[-noNuvirdi,]
dim(data) == dim(na.omit(data))
```

Now we'll check whether any of the numerical variables are highly correlated. 

```{r, eval = F}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,ntm2,svalm2,geymm2))
cor(X)
```

The $\texttt{ibm2}$ and $\texttt{ntm2}$  have a very high correlation so we removed one of them. 

```{r, eval = F}
data$ntm2 = NULL 
```

Now the data is ready for visualization. 
