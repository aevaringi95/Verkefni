---
title: "Final Project"
author: "Rakel María Brynjólfsdóttir og Ævar Ingi Jóhannesson"
date: "March 23, 2018"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

```{r, warning = F, message = F}
library(plyr); library(dplyr); library(lubridate)
library(ggplot2)
library(grid)
library(gridExtra)
library(glmnet)
library(e1071)
library(randomForest)
```

We start by loading the data which has already been cleaned. The particulars of the data preproccesing can be seen in the appendix. 

```{r}
data = read.csv("MyData.csv")
```

```{r, echo = F}
#laga factor, prentast ekki
data$fjmib = as.factor(data$fjmib)
data$lyfta = as.factor(data$lyfta)
data$fjhaed = as.factor(data$fjhaed)
data$fjbkar = as.factor(data$fjbkar)
data$fjsturt = as.factor(data$fjsturt)
data$fjklos = as.factor(data$fjklos)
data$fjeld = as.factor(data$fjeld)
data$fjherb = as.factor(data$fjherb)
data$fjstof = as.factor(data$fjstof)
data$matssvaedi = as.factor(data$matssvaedi)
data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
data$ibteg = as.factor(data$ibteg)
```

The data is now as follows: 

  Variable     |  type     | Description               |  levels
-------------- | --------- | ------------------------- | ----------------------- 
kdagur         | numerical | days since last bought    | 
nuvirdi        | numerical | log of current price      | 
teg_eign       | factor    | type of property          | Einbýlishús, Fjölbýlishús, Íbúðareign, Íbúðarhús, Raðhús
svfn           | factor    | town id                   | Grb, Hfj, Kop, Mos, Rvk, Sel
aldur          | numerical | age of house              |   
fjmib          | factor    | nr of apartments          | 1,2,3,4
lyfta          | factor    | elevator or not           | 0,1
ummal          | numerical | circumference of property | 
ibm2           | numerical | area of property          |
ntm2           | numerical | netto area of property    |
fjhaed         | factor    | number of floors          | 1,2,3,4
fjbkar         | factor    | number of bathtubs        | 0,1,2 (2 or more)
fjsturt        | factor    | number of showers         | 0,1,2 (2 or more)
fjklos         | factor    | number of toilets         | 0,1,2 (2 or more)
fjeld          | factor    | number of kitchens        | 0,1,2 (2 or more)
fjherb         | factor    | number of rooms           | 0,1,2,3,4,5,6 (6 or more)
fjstof         | factor    | number of livingrooms     | 0,1,2 (2 or more)
stig10         | numerical | construction stage        |
bilskurm2      | numerical | area of garage            |
svalm2         | numerical | area of balcony           |
geymm2         | numerical | area of storage space     |
matssvaedi     | factor    | location                  | 60 levels
undirmatssvaedi| factor    | sub-location              | 62 levels
ibteg          | factor    | type of property          | 1(single-family), 2(aparment building)

### Descriptive plots of the data

We start by looking at the density of the $\texttt{nuvirdi}$ variable.  

```{r}
density1 <- ggplot(data,aes(nuvirdi))+
  geom_density()

density2 <- ggplot(data,aes(log(nuvirdi)))+
  geom_density()

grid.arrange(density1,density2,nrow=2)
```

It's evident that when we plot $\texttt{nuvirdi}$ on a log scale it looks a lot more like the normal distribution, so we're going to log transform the response. 

```{r}
data$nuvirdi = log(data$nuvirdi)
```


Let's plot $\texttt{nuvirdi}$ against some variables to see if there are any relationships. 

```{r}
p1 <- ggplot(data, aes(x=kdagur,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey()

p2 <- ggplot(data, aes(x=stig10,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p3 <- ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p4 <- ggplot(data, aes(x=ummal,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p5 <- ggplot(data, aes(x=aldur,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p6 <- ggplot(data, aes(x=bilskurm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p7 <- ggplot(data, aes(x=svalm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

p8 <- ggplot(data, aes(x=ibm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey()

grid.arrange(p1,p2,p3,p4,nrow = 2)
grid.arrange(p5,p6,p7,p8,nrow = 2)
```

It looks like $\texttt{geymm2}$, $\texttt{bilskurm2}$ and $\texttt{ibm2}$ might have a more linear relatioship with nuvirdi on a log scale. 

```{r}
B = data$bilskurm2
ind1 = which(B == 0)
B[-ind1] = log(B[-ind1])
I = log(data$ibm2)
G = data$geymm2
ind3 = which(G == 0)
G[-ind3] = log(G[-ind3])
```

```{r}
p1 <- ggplot(data, aes(x=B,y=nuvirdi)) +
          geom_point(size=0.5)+
          theme_grey() + labs(x="bilskurm2")

p2 <- ggplot(data, aes(x=I,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="ibm2")

p3 <- ggplot(data, aes(x=G,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() + labs(x="geymm2")

grid.arrange(p1,p2,p3,nrow = 2)
```

A log transformation is not suitable for the $\texttt{bilskurm2}$ variable, but it definitely is for $\texttt{ibm2}$ and $\texttt{geymm2}$, although we will have to fix the values that are at zero. 

```{r}
data$ibm2 = log(data$ibm)

G[ind3] = G[ind3] - 1.8 
data$geymm2 = G
ggplot(data, aes(x=geymm2,y=nuvirdi)) +
          geom_point(size=0.5) +
          theme_grey() 
```


Now we'll make some box plots of the factor variables. 

```{r}
f1 <- ggplot(data,aes(x=teg_eign,y=nuvirdi))+
          geom_boxplot()

f2 <- ggplot(data,aes(x=svfn,y=nuvirdi))+
          geom_boxplot()

f3 <- ggplot(data,aes(x=fjmib,y=nuvirdi))+
          geom_boxplot()

f4 <- ggplot(data,aes(x=lyfta,y=nuvirdi))+
          geom_boxplot()

f5 <- ggplot(data, aes(x=fjhaed,y=nuvirdi)) +
          geom_boxplot()

f6 <- ggplot(data,aes(x=fjbkar,y=nuvirdi))+
          geom_boxplot()

f7 <- ggplot(data,aes(x=fjsturt,y=nuvirdi))+
          geom_boxplot()

f8 <- ggplot(data,aes(x=fjklos,y=nuvirdi))+
          geom_boxplot()

f9 <- ggplot(data,aes(x=fjeld,y=nuvirdi))+
          geom_boxplot()

f10 <- ggplot(data,aes(x=fjherb,y=nuvirdi))+
          geom_boxplot()

f11 <- ggplot(data, aes(x=fjstof,y=nuvirdi)) +
          geom_boxplot()

f12 <- ggplot(data, aes(x=ibteg,y=nuvirdi)) +
          geom_boxplot()

grid.arrange(f1,f2,f3,
             f4,f5,
             f6,f7,
             f8,f9,f10,f11,f12,ncol=3)
```

Most of the factor variables seem to have a relationship with $\texttt{nuvirdi}$, especially $\texttt{fjmib}$, $\texttt{fjhaed}$ and $\texttt{fjherb}$. 

Finally, we'll check the correlation between the numerical variables. 

```{r}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,svalm2,geymm2))
cor(X)
```

None of the variables are highly correlated. 

### Modeling the data


When exploring the data we realised that the $\texttt{svfn}$ variable and the two $\texttt{matssvaedi}$ variables are linearly dependent since they are describing the same thing. For the regression part we decided to use the $\texttt{matssvaedi}$ variables and for the classification we are going to use the $\texttt{svfn}$ variable. 

```{r}
set.seed(8756)
dat = data
svfn = dat$svfn
dat$svfn = NULL
```

Now we'll split the data into a training set with 80% of the data and a test set. 

```{r}
n = dim(dat)[1]
train = sample(n,floor(n*0.8)) 
dat.train = dat[train,]
dat.test = dat[-train,]
```

## Linear Regression

To start we are using linear regression to predict (the log of) $\texttt{nuvirdi}$ 

```{r}
fit.lm = lm(nuvirdi~.,dat.train)
summary(fit.lm)
```

All of the numerical variables are significant and most of the factors have significant levels, apart from $\texttt{fjbkar}$ and $\texttt{fjklos}$.

Er hægt að sjá hvort reference levelið sé significant?


```{r}
pred.lm = predict(fit.lm, dat.test)
RSS1 = mean((dat.test$nuvirdi - pred.lm)^2)
Rss = mean((exp(dat.test$nuvirdi)-exp(pred.lm))^2)
Rss
RSS1
```

# Diagnostics of linear regression
Here below we can see some plots for the linear regression model.

```{r}
diagnost = fortify(fit.lm)
diagnost$.index = 1:(dim(diagnost)[1])
res_sd = sd(diagnost$.resid)
resPlot = ggplot(diagnost, aes(x=seq(1:length(.resid)),y=.resid)) +
                geom_point()+
                geom_hline(yintercept=0, col="red", linetype="dashed")+
                geom_hline(yintercept=2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=-2*res_sd, col="blue", linetype="dashed")+
                geom_hline(yintercept=3*res_sd, col="green", linetype="dashed")+
                geom_hline(yintercept=-3*res_sd, col="green", linetype="dashed")+
                xlab("Index")+ylab("Residuals")+labs(title = "Index plot of residuals")+
                geom_text(aes(label=ifelse(abs(.resid)>3*res_sd,.index,"")),
                      hjust=-.5, vjust=0.4)+
                theme_grey()
resPlot
```

Here we can see the residuals plotted against their index. The blue line is 2 times the standard deviation of the residuals and the green line is 3 times the standard deviation. We can see that there are few points that exceed the green line and they could be looked at more closely. Also we can see that there seems to be a constant variance.

```{r}
QQPlot<-ggplot(diagnost, aes(sample = .resid))+
        stat_qq()+
        geom_abline(slope=res_sd)+
        ylab("Residuals")+
        theme_grey()
QQPlot
```

Here we can see the normal Q-Q plot for the model. It looks good and they seem to follow a normal distribution but the ends are more spread out. They are heavi-tailed.

```{r}
LevPlot <- ggplot(diagnost, aes(x=seq(1:length(.hat)),y=.hat))+
                  geom_point()+
                  geom_hline(yintercept=4*mean(diagnost$.hat), col="red", linetype="dashed")+
                  xlab("Index")+
                  ylab("Leverages")+
                  geom_text(aes(label=ifelse(.hat > 4*mean(diagnost$.hat),.index,"")),hjust=0, vjust=0)+
                  theme_grey()
LevPlot
```

Here we can see the Leverage plot. There seems to be one point that has very high leverage. This point is in undirmatssvæði 101 which has only 2 observations and there for it has this high leverage. So it doesn't mean that it's an outlier.



##Best subset selection.

Here we can see that R adjusted.
```{r,eval = F}
library(leaps)
regfit.fwd <- regsubsets(nuvirdi ~ ., data = dat.train, 
    nvmax = 150,method="forward")
for.summary <- summary(regfit.fwd)
which.max(for.summary$adjr2)

ggplot(data.frame(y=for.summary$adjr2, x=1:150), aes(x,y))+geom_point() + ylab("AdjR2") + xlab("Subset size")
```


## Lasso Regression

Mögulega þarf að scale-a breytti ekki miklu samt. Lasso er samt frekar lelegt miðað við linear.
```{r,eval = F}
ytrain = data.matrix(dat.train[,"nuvirdi"])
Xtrain = data.matrix(dat.train[,!(colnames(dat.train) %in% c("nuvirdi"))])
Xtest = data.matrix(dat.test[,!(colnames(dat.train) %in% c("nuvirdi"))])
grid <- 10^seq(1, -20, length=1000)
fit.lasso = cv.glmnet(Xtrain,ytrain,alpha=1, lambda=grid, thresh=1e-12)
plot(fit.lasso)
best.lambda = fit.lasso$lambda.min
pred.lasso = predict(fit.lasso,Xtest,s=best.lambda)
RSS2 <- mean((dat.test$nuvirdi - pred.lasso)^2)
RssNormal2 <- mean((exp(dat.test$nuvirdi)-exp(pred.lasso))^2)
RSS2
RssNormal2

```

Here we can see that when we do Lasso regression we can see that the MSE is much higher then for a normal linear regression.


```{r,eval = F}
best.model = glmnet(Xtrain, ytrain, alpha = 1)
predict(best.model, s = fit.lasso$lambda.1se, type = "coefficients")
```

The first two predictors to tend to zero are fjeld and svalm2.


##Boosting

Here we perform Boosting.

```{r,eval = F}
library(gbm)
set.seed(50550)
lambd <-  seq(0.001, 0.02, by=0.001)
lambd <- c(lambd, seq(0.02, 0.5, by=0.01))
m <- length(lambd)
testErr <- rep(NA, m)
for (i in 1:m) {
    boostCol <- gbm(nuvirdi ~ ., data = dat.train,
                    distribution = "gaussian",
                    n.trees = 200, 
                    shrinkage = lambd[i])
    testPred <- predict(boostCol, dat.test, n.trees = 200)
    testErr[i] = mean((dat.test$nuvirdi - testPred)^2)
    print(i)
}

bestLam = lambd[which.min(testErr)]

ggplot(data.frame(x=lambd, y=testErr), aes(x=x, y=y)) +
      xlab("Shrinkage") + 
      ylab("Test MSE") + 
      geom_point()


boost.fit <- gbm(nuvirdi ~ ., data = dat.train, distribution = "gaussian", n.trees = 1000, shrinkage =bestLam )
boost.pred <- predict(boost.fit, dat.test, n.trees = 1000)
RssBoos = mean((dat.test$nuvirdi - boost.pred)^2)
Rss2Boost = mean((exp(dat.test$nuvirdi) - exp(boost.pred))^2)

Rss2Boost
RssBoos
```




### Classification 

Here we are going to predict in which town(Svfn) the properties belong to. We start by taking only properties from Reykjavik,kopavogi and Hafnarfirði.
The classes are 
Rvk
Kop
Hfj

```{r}
str(dat)
dat = data.frame(dat,svfn)
mats = dat$matssvaedi
undir = dat$undirmatssvaedi
dat$matssvaedi = NULL
dat$undirmatssvaedi = NULL

area = levels(data$svfn)[c(1,2,5)]
dat = dat[which(data$svfn %in% area),]
dat$svfn = droplevels(dat$svfn)

n = dim(dat)[1]
p = dim(dat)[2]
train = sample(n,floor(n*0.8))
dat.train = dat[train,]
dat.test = dat[-train,]

n2 = dim(dat.train)[1]
val = sample(n2,floor(n2*0.8))
dat.train = dat.train[val,]
dat.val = dat.train[-val,]

```

Here we take out Matssvæði and undirmatsvæði because they are describing the same thing as Svfn.
We also split to a train set, test set and a validation set to use in cross validation for SVM.

```{r}
table(dat$svfn)
```

We can see that most of the properties are in Reykjavik.

## Random Forest

First we use Random Forest to try and classify into these three groups.

We start by finding the number m which is the sample of random predictors that is chosen as split candidates in each split.

```{r}
library(randomForest)
set.seed(4949)
err = integer()
mtrys = seq(from = 2, to = 20, by = 1)
for(i in mtrys){
  fit.rf = randomForest(svfn~., dat.train, mtry = i, ntree = 200) 
  pred.rf = predict(fit.rf, dat.test)
  err[i-1] = mean(dat.test$svfn != pred.rf)
  print(i)
}

ggplot(data=data.frame(x=mtrys,y=err),aes(x=x,y=y))+
  xlab("nr of variables considered at each split")+
  ylab("MSE")+
  geom_point()

```

Here we can see that it seems that 17 variables gives the lowest MSE. Let's fit a model using 1000 trees with $m=17$.


```{r}
minErr = which.min(err)
fit.rf = randomForest(svfn~., dat.train, mtry =minErr+1, ntree = 1000)
pred.rf = predict(fit.rf, dat.test)
misErr = mean(dat.test$svfn != pred.rf)

misErr
```

Here we can see that the missclassification error rate is:  11.36% 


```{r}
fit.rf
importance(fit.rf) 
```

Here we can see both the confusion matrix and the most important variables for the classification.

The most important variables are:
Aldur
Ummal
Ibm2
Nuvirdi



## Support Vector Machine

Here we use SVM without a kernel and with radial kernel to try and classify. The svm function from the e1071 library uses the One-versus-one classification. We start by using the validation set to choose C and then $\gamma$ for the radial kernal. We also scale the data for the SVM because it gave a better result.

```{r}
library(e1071)
set.seed(5040)

tune.svm = tune(svm, svfn~.,
                data = dat.val,
                scale=T, 
                ranges = list(cost = c(50,100,200,300)))  
summary(tune.svm)


fit.svm = svm(svfn~.,
              data = dat.train,
              scale = T,
              cost = 200)
pred.svm = predict(fit.svm,dat.test)
err.svm = mean(pred.svm!=dat.test$svfn)
err.svm
```


Here we don't use a kernel for our SVM and get the missclassification error rate :


```{r}
set.seed(5040)
tune.svm2 = tune(svm, svfn~.,
                data = dat.val,
                scale=T,
                kernel="radial",
                ranges = list(gamma=c(0.01,0.1,0.5),cost = c(50,100,200,300))) 
summary(tune.svm2)


fit.svm2 = svm(svfn~.,
              data = dat.train,
              scale = T,
              kernel="radial",
              gamma = 0.1,
              cost = 50)
pred.svm = predict(fit.svm2,dat.test)
err.svm2 = mean(pred.svm!=dat.test$svfn)
err.svm2

table(pred.svm,dat.test$svfn)
```

Here we use radial kernel and get..

### Appendix

###Data preprocessing

```{r}
data = read.csv("gagnasafn_ibudarhusnaedi.csv", header = T, sep = ";", dec = ",")
```


We start by removing all the data which is not in the capital region. 

```{r, eval = F}
capitalArea = c(0000,1000,1100,1300,1400,1604,1606)
data = data[data$svfn %in% capitalArea,]
```

Now we are going to remove the variables that we won't be using.

```{r, eval = F}
delete = names(data)[c(1,2,5,6,7,10,11,12,13,14,15,17,18,19,20,21,
                       25,26,30,37,38,40,41,42,44,47,50,52,53)]
data = data[,-which(names(data) %in% delete)]
```

Some variable have many levels and few observations on some levels so we are going to remove those observations. 

```{r, eval = F}
#Einbylishus, Fjolbylishus, Ibudareign, Ibudarhus, Parhus, Radhus. Getum fækkað meira 
lev = levels(data$teg_eign)[c(1,2,7,8,11)] 
data = data[which(data$teg_eign %in% lev),]
data$teg_eign = droplevels(data$teg_eign)

data$matssvaedi = as.factor(data$matssvaedi)
lev = levels(data$matssvaedi)[c(61,62)]
data = data[-which(data$matssvaedi %in% lev),]
data$matssvaedi = droplevels(data$matssvaedi)

data$undirmatssvaedi = as.factor(data$undirmatssvaedi)
lev = levels(data$undirmatssvaedi)[c(10,36,38,39,50,55)]
data = data[-which(data$undirmatssvaedi %in% lev),]
data$undirmatssvaedi = droplevels(data$undirmatssvaedi)
```

Many of the numerical variables should be factors, and some factors should be numerical. We are also going to combine some of the levels for certain factors since the higher levels have very few observations in many cases.

```{r, eval = F}
data$svfn = as.factor(data$svfn)
data$lyfta[which(data$lyfta != 0)] = 1
data$lyfta = as.factor(data$lyfta)
data$fjsturt[which(!(data$fjsturt %in% c(0,1)))] = 2
data$fjsturt = as.factor(data$fjsturt)
data$fjklos[which(!(data$fjklos %in% c(0,1)))] = 2
data$fjklos = as.factor(data$fjklos)
data$fjbkar[which(!(data$fjbkar %in% c(0,1)))] = 2
data$fjbkar = as.factor(data$fjbkar)
data$fjeld[which(!(data$fjeld %in% c(0,1)))] = 2
data$fjeld = as.factor(data$fjeld)
data$fjstof[which(!(data$fjstof %in% c(0,1)))] = 2
data$fjstof = as.factor(data$fjstof)
data$fjhaed = as.factor(data$fjhaed)
data$fjherb[which(!(data$fjherb %in% c(0,1,2,3,4,5)))] = 6
data$fjherb = as.factor(data$fjherb)
data$svalm2 = as.numeric(data$svalm2)
data$fjmib = as.factor(data$fjmib)
data$ibteg = as.factor(data$ibteg)
data$svfn = revalue(data$svfn, c("0"="Rvk","1000"="Kop",
                                 "1100"="Sel","1300"="Grb",
                                 "1400"="Hfj","1604"="Mos"))
data$ibteg = revalue(data$ibteg, c("11"="1","12"="2"))
data$byggar = droplevels(data$byggar)
```

We are going to change the kdagur variable so that we have the days since the property was bought. 

```{r, eval = F}
data$kdagur <- strptime(x = as.character(data$kdagur),
                                format = "%d.%m.%Y")
mydate <- as.Date("2018-04-01")
data$kdagur <- as.Date(data$kdagur)
data$kdagur <- as.numeric(mydate-data$kdagur)
```

And we're going to change the byggar variable into the age of the house in years. 

```{r, eval = F}
data$byggar <- 2018 - as.numeric(as.character(data$byggar))
colnames(data)[5] = "aldur"
```

Now we remove 0 values for nuvirdi and check for NA values. 
```{r, eval = F}
noNuvirdi <- which(data$nuvirdi==0)
data = data[-noNuvirdi,]
dim(data) == dim(na.omit(data))
```

Now we'll check whether any of the numerical variables are highly correlated. 

```{r, eval = F}
X = with(data,cbind(kdagur,aldur,ummal,ibm2,stig10,bilskurm2,ntm2,svalm2,geymm2))
cor(X)
```

The ibm2 and ntm2 have a very high correlation so I'm removing one of them. 

```{r, eval = F}
data$ntm2 = NULL 
```

